---
date: 09-01-2025
title : Satellite Imaging, Construction of a 3D Mesh from Satellite Images of Urban Scenes
tags: ["Deep Learning"]
stags: ["Deep Learning"]
links: ["https://github.com/gardiens/MVA_24_25/tree/main"]
linksDescription: ["Github repo"]
image: "image.png"
---

## Abstract
Our work focused on creating a colored mesh (mesh) from various satellite images of a location. The approach we followed extends the work done in the course's tutorials: taking pairs of images, (i) rectifying them, (ii) calculating their disparities (stereo-matching), and triangulating the 3D positions associated with each pixel to construct (iii) a point cloud or (iii') a 2.5D elevation map by flattening these points onto a grid.
The first method involves forming the mesh's triangles based on the connectivity in the initial image. We know the xyz coordinates of each pixel in image 1, for which we found the equivalent in image 2 (allowing us to triangulate its position). For each triplet of neighboring pixels in the image (e.g., at coordinates $(i,j)$, $(i+1,j)$, and $(i,j+1)$ for any $i$ and $j$), we can connect the corresponding xyz points (provided we have found the xyz coordinates of all three pixels). The second method performs a similar operation but relies on 2.5D Digital Elevation Models (DEMs) calculated at the end of the previous tutorial. The mesh coloring is achieved through z-buffering or rasterization by projecting each point of the cloud onto the scene's images. In both methods, we can construct a mesh from a single pair of images or multiple pairs by fusion. Additionally, using Multi-Spectral Images in addition to our initial grayscale images, we colored the meshes in RGB.

#  Satellite Imaging: Construction of a 3D Mesh from Satellite Images of Urban Scenes
ddition to our initial grayscale images, we colored the meshes in RGB.

## Bibliography

The tutorials followed are from the course *Remote sensing data: from sensor to large-scale geospatial data exploitation*, following the S2P pipeline [1], and the approach described in [2]. To compare with the developments in this report, we tested a standard mesh construction method from a point cloud. Using the software [Cloud Compare](http://www.cloudcompare.org/) (seen in our point cloud course), it is possible to construct a mesh through Poisson surface reconstruction, as shown in Figure 1. This method relies on estimating normals at each point, calculated here using the 50 nearest points to each point.


### Other Recent Methods, via NeRF or 3D Gaussian Splatting

The success of NeRF [3] and 3D Gaussian Splatting [4] methods for generating new views of a scene opens possibilities for processing satellite images for environment reconstruction. Approaches like Sat-Mesh [5] and EO-Nerf [6] adapt NeRFs to satellite images, obtaining much higher quality results than the methods studied in this report, but at a much slower pace. In the Point Cloud course project (NPM3D), we studied Point-NeRF [7], one of the papers aiming to address the limitation of NeRFs and Gaussian Splatting: the need to reoptimize independently for each scene. The idea of Point-NeRF [7], MVS NeRF [8], and MVS Gaussian [9] is to pre-train a model to generate an initial representation of a scene, followed by per-scene optimization. A new scene can thus be processed much faster. MVS-Gaussian [9] claims to process a scene in less than a minute; however, their approach seems limited to generating views at angles close to the provided images' views (whereas the goal for satellite images is to generate views from the ground, perpendicular to the satellite views). These approaches rely on a deep network for Multi-View Stereo (MVS). Point-NeRF [7] deduces a point cloud and attaches a feature vector to each point via a 2D CNN on the images. The ray marching rendering network is also pre-trained since it is local (relying on the feature vectors of points near the ray) and not global (a single large MLP per scene). Pixel Splat [10] is another approach that pre-trains a network to quickly process a new scene via splatting. These methods seem particularly suited for satellite imagery, as processing a vast geographical area (e.g., an entire city) involves dividing the area into scenes with very similar local compositions. Thus, the pre-trained ray marching network should quickly yield good results for each scene. The point cloud construction can be done as learned during our course (whereas 3D Gaussian Splatting relies on [Colmap](https://colmap.github.io/)).

## Method 1 - Connectivity in the Image

The first implemented method is illustrated in Figure 2, with results shown in Figure 3. Stereo vision, as seen in the course, constructs a disparity map between two images. From this matrix, we triangulate the positions of pixels from image 1 for which we found the equivalent in image 2. By masking pixels for which we could not construct the corresponding point, we obtain a matrix as shown in Figure 2a. The idea is to connect triplets of neighboring pixels. Three neighboring pixels on the image have a good chance of belonging to the same surface element (building, tree, car, etc.), except at the border between two elements (see problem in image 3b). Thus, we initially pave the matrix with triangles, avoiding missing pixels, as shown in Figure 2c and result 3a. We also retrieve the colors of the points, associating pixels (matrix values, i.e., the image) with points. We then filter the triangles based on two rules: (1) Taking the longest edge of a triangle as its representative length, we filter out triangles with representative lengths 5 times greater than the median of the representative lengths. (2) We calculate a normal for each triangle $(A,B,C)$ via the cross product $BA \otimes CA$ and the incidence angle of the image. We then calculate the angle between the triangle's normal and the image's normal. If the angle is greater than 88° (or less than 88°), we consider it too close to perpendicular (90°). A triangle perpendicular to the image's incidence angle has a high risk of being an aberration. Unfortunately, this leads to a defect in this first method: the mesh is very patchy, especially on the rear facades of buildings. For an order of magnitude, in the example of image 3b, the number of triangles decreases from 563,040 to 555,461 after filtering by size, then to 541,915 after filtering by the normal's angle; see image 3c after filtering and images 3d and 3e for two colored viewpoints.



### Using Multiple Images

This method relies exclusively on the image for which we calculated the disparity map with another, so we cannot use multiple images in the mesh construction. However, as suggested by Gabriele Facciolo during the oral presentation of our work, we can superimpose meshes obtained from different image pairs. After a first attempt, we noticed that this superimposition partially addressed the issue of missing facades. The xy coordinates of the different point clouds are fairly well-aligned, but the z (or h) coordinates are not, as the elevation is relative (see image 4a). To roughly align the different meshes, we subtract the mean elevation of each mesh from the elevation of each point in that mesh, as shown in result 4b. The xy coordinates are fairly well-aligned, but this is not perfect, and we observe a spreading of the buildings (see image 4c). To limit the spreading, we can restrict the number of image pairs used, as shown in image 4d using only pairs relative to the same image. The results obtained by this method have the advantage of a dense point cloud and thus better resolution than Method 2. However, the combination of multiple meshes would need improvement (in essence, Method 2 aims to combine point clouds by projecting points onto an elevation grid). Despite superimposing all selected image pairs (see details below), many facades remain missing, especially on the central tower of the scene. We, unfortunately, use images with roughly the same view of the scene, seeing the same facades. We also notice noise in the mesh, more visible with the superimposition of multiple meshes and some aberrations in the air.



### Details on the Method

- For each square of coordinates $(i,j)$, $(i+1,j)$, $(i,j+1)$, and $(i+1,j+1)$, if none are missing, we have two choices for constructing the pair of triangles that will pave the square: either $<p_{(i,j)}, p_{(i+1,j)}, p_{(i,j+1)}>$ with $<p_{(i+1,j)}, p_{(i+1,j+1)}, p_{(i,j+1)}>$ or $<p_{(i,j)}, p_{(i+1,j)}, p_{(i+1,j+1)}>$ with $<p_{(i,j)}, p_{(i+1,j+1)}, p_{(i,j+1)}>$ (where $p_{((a,b))}$ is the point in the cloud associated with pixel $(a,b)$). To decide, we calculate the length of the two diagonals of the square: $d_0 = ||p_{(i,j)} - p_{(i+1,j+1)}||_2$ and $d_1 = ||p_{(i+1,j)} - p_{(i,j+1)}||_2$. We keep the pair of triangles that gives the smallest diagonal (see `technique1.py` line 87).
- All triangles are oriented clockwise so that all faces are rendered with the same shading.
- Our implementation is not specific to a set of images, and at the beginning of our Jupyter Notebook, you can choose between the Tokyo scene from TP7 and the IARPA images from Argentina from previous TPs. However, we focused our tests solely on the IARPA images. To work with images of the same luminosity (so that stereo matching works), we first tried filtering the images based on the attributes "NITF_USE00A_ROLL_ANG" and "NITF_CSEXRA_SUN_AZIMUTH". Following our supervisors' advice, we focused on the images from December 18, 2015, corresponding to numbers 37 to 42 (sorted by acquisition date). Image 40 did not fully cover the area (or we could not crop it with the aoi), so we removed it. Thus, we worked with 5 images, resulting in 10 different pairs. For versions with a single pair, we used pair (37,38).
- To obtain RGB colored meshes instead of grayscale, we perform pansharpening. On the IARPA images, in addition to high-resolution images, we recovered lower-resolution but multi-spectral MSI images (which also explains our focus on the IARPA scene in Argentina). The implementation is in the file `utils_color.py` and follows a tutorial sent by our supervisors: (i) Converting WorldView-3 spectra to RGB colors, then (ii) zooming via Fourier transform, and finally (iii) combining with high-resolution images (in grayscale). Note: the crop of aoi between the high-resolution image and the corresponding MSI is adjusted via a dictionary of differences, the `dict_diff` introduced in `rectification.affine_crop`. Results are shown in Figure 5.
- The display of meshes is done with MeshLab software, with shaders disabled. Our supervisors showed us how to use MeshLab to increase the resolution of a mesh, but we did not use these features. This choice is partly due to time constraints and partly to maintain clarity regarding our implementations and results. Improving the mesh through refinement would be additional work beyond what we have done.
- A minor detail regarding MeshLab: to avoid mesh flickering during display, we recentered the xyz coordinates of the mesh (by subtracting a random point). This is necessary because MeshLab stores coordinates in absolute terms, not relative, and with too few bytes. The x and y values hover around $3 \times 10^5$ and $6 \times 10^6$, respectively, but vary only slightly between points. Without recentering, the coordinates are rounded due to insufficient bytes, causing points to jump between displays.


## Method 2 - Connectivity in the Elevation Map

The results of the second method are shown in Figure 6. This method uses 2.5D DEMs, elevation maps produced at the end of the previous tutorial. These maps are calculated by projecting the points of the cloud onto a grid along the elevation axis. This method naturally allows combining multiple image pairs. (1) The first possible fusion consists of assembling all points into a single large cloud and then constructing the DEM of this cloud. (2) The second fusion, the chosen one, consists of taking the median of the elevation maps (nanmedian to fill gaps), as shown in Images 6a to 6e (ignore the color scales). The resulting DEM is a matrix with elevation values at the grid coordinates and missing values. We can then apply the tool developed for the first method (literally the same function, with the same details) to construct the mesh, with the advantage of being able to fill gaps in the elevation map through interpolation. We use Neumann interpolation, as shown in image 6f for the interpolated DEM. Additionally, we do not need to filter the mesh (by size or normal angles). Thus, we obtain a mesh without holes.


### Colorization by Rasterization

Unlike the first method, the matrix used to construct the mesh is not the image but the elevation map (in practice, we only use the coordinates of the points corresponding to each index and the missing cells), with the significant drawback of not having a color for each point. Colorization is achieved through rasterization on any image. This step can be performed independently on multiple images, combining the colors afterward.


- Given an image (not necessarily one used to generate the mesh), we use the RPC to project all points of the cloud onto the pixels.
- A point in the shadow of another (e.g., the hidden facade of a building) is not visible in the image, so the found pixel does not belong to it. To color only the points visible in the image, we tested two solutions: z-buffering and rasterization. Since z-buffering was not conclusive, we abandoned it (see the code `technique2.OLD_project_color`).
- **Rasterization** (illustrated in Figure 7): In the order of depth (following the image's incidence angle), we rasterize the triangles. Rasterization involves coloring the points included in each 2D triangle (based on the coordinates projected onto the image), superimposing the triangles one by one. For an efficient implementation, we use `rasterio`. We thus obtain a collage of triangles covering the coordinates of the image's points. A point in the cloud hidden by a triangle of which it is not a vertex is not colored.

### Colorization with Multiple Images

Rasterization on an image colors the points visible in that image. To aid understanding, we represent uncolored points in purple. Image 8a is a top-down view of the colorization obtained by projection onto image 37, and image 8b is colored by image 42. By taking the maximum of the colors of each point, we can cover the uncolored areas, as shown in image 8c. Taking the maximum ( rather than the average) helps to brighten the shaded areas. Thus, by multiplying the colorizations, we reveal the shaded areas left by the sun. For example, in image 8a, 24,614 points are colored (out of 
