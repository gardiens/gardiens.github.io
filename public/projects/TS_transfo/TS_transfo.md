---
date: 07-03-2024
title : Transformers for Time Series
tags: ["FEDformer, Times Series,Skeleton,Computer Visions"]
stags: ["Time Series","Computer Visions"]
links: ["https://github.com/gardiens/Time-Series-Library_skeleton"]
linksDescription: ["Github"]
image: "gif.gif"
---

## Abstract

A project to solve to apply Time Series Transformers to prediction of human Skeleton. 
In particular, we focus on promising architecture such as FEDFormer and AutoFormer.
We submitted to JDSE 2023 and the paper can be found [here](https://github.com/gardiens/Time-Series-Library_skeleton/blob/master/9_Investigating_Transformers_f.pdf)


## Motivation 

The study at hand exists within the broader framework of identifying neurological pathologies in newborns through the analysis of spontaneous motor skills. We aim to develop pre-diagnostic tools for early detection. The central goal of this study
is to assess transformer-based models’ accuracy in forecasting motor skills for individuals without pathologies (normal data). We anticipate neurological abnormalities
(abnormal data) to disrupt movement patterns, heightening prediction complexity.
Presently, an inadequate number of videos capturing newborns’ motor activities are available for analysis. Thus, we turned our attention to the NTU RGB+D
dataset[2]. This dataset emerged as a suitable substitute for our research objectives.


## Methods 
We conducted various tests involving dataset modifications, preprocessing steps,
and model enhancements. As we cannot present all the experiments, we focus here
on FEDformer and specific preprocessing steps that yielded the best results.
Dataset We used the NTU RGB+D[2] dataset, which features motion sequences of
adults spanning durations of 2 to 10 seconds. Each 3D skeletal data contains the 3D
coordinates of 25 body joints at each frame.
Preprocessing Each data is normalized by removing the mean of each time series.
We divided the data into input and output segments, each consisting of 16 frames
(approximately 1 second) at a precise point corresponding to a change in movement.
Model FEDformer implementation was taken from TSLib library1.
Experimental Setup All computational tasks were performed within the LabIA2
computing environment at the University of Paris-Saclay. Using the given input data
for training, the model is required to predict both the input and output sequences.
Evaluation Metrics We adopted widely-used metrics such as Mean Squared Error
(MSE) and Mean Absolute Error (MAE). We also conducted visual evaluations by
comparing the ground truth movement patterns with the predictions generated by our
models


## Results 
showcases the performance of FEDformer both with and without specific
preprocessing steps aimed at enhancing its efficacy in human action forecasting. Visual
outputs are available, along with our code, on GitHub.
The table illustrates the enhancement in FEDformer’s performance due to our preprocessing interventions. In terms of the training set, we observed a 1.7-fold reduction
in MSE and a 20-fold decrease in MAE.
Nevertheless, upon visual inspection of the results, we noticed that the model
encountered difficulties in accurately predicting substantial movements. Its predictions
extended the termination of input movements, presenting a misalignment with actual
patterns.
We may explore alternative metrics and evaluation methodologies to gain insights
into the underlying reasons for the model’s suboptimal learning.

